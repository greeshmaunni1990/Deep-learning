{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 dataset usisng Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load cifar10 data\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "\n",
    "# check the shape of data\n",
    "print('shape of trainX is {} and shape of trainy is {}'.format(trainX.shape, trainy.shape))\n",
    "print('shape of testX is {} and shape of testy is {}'.format(testX.shape, testy.shape))\n",
    "\n",
    "# plot some images\n",
    "for i in range(9):\n",
    "    plt.subplot(330 +1 + i)\n",
    "    plt.imshow(trainX[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Test Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# First load dataset\n",
    "def load_dataset():\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    \n",
    "    # one Hot encoding the target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# second scale the pixels\n",
    "def prep_pixels(trainX, testX):\n",
    "    \n",
    "    # Convert the type from usigned int to float\n",
    "    train_norm = trainX.astype('float32')\n",
    "    test_norm = testX.astype('float32')\n",
    "    \n",
    "    # normalise the values\n",
    "    train_norm = train_norm/255.0\n",
    "    test_norm = test_norm/255.0\n",
    "    \n",
    "    return train_norm, test_norm\n",
    "\n",
    "# Third Define model\n",
    "def define_model():\n",
    "    \n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fourth plot the learning curve\n",
    "def summarize_diagnostics(history):\n",
    "    \n",
    "    # define subplot\n",
    "    plt.subplot(221)\n",
    "    # plot loss\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    \n",
    "    # plot accuracy\n",
    "    plt.subplot(222)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['acc'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_acc'], color='orange', label='test')\n",
    "    \n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    plt.savefig(filename + '_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "# Run the harness test for evaluating the model\n",
    "def run_harness():\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    \n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    \n",
    "    model = define_model()\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('accuracy is {}'.format(acc))\n",
    "    \n",
    "    summarize_diagnostics(history)\n",
    "    \n",
    "run_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with one VGG Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aifi/anaconda3/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4d5c0acbd210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0msummarize_diagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# entry point, run the test harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4d5c0acbd210>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX,\n\u001b[0;32m---> 70\u001b[0;31m     testY), verbose=0)\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=[ 'accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['acc'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_acc'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX,\n",
    "    testY), verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with 2 VGG Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/aifi/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/aifi/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/aifi/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/aifi/anaconda3/lib/python2.7/inspect.py\", line 1058, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/home/aifi/anaconda3/lib/python2.7/inspect.py\", line 1018, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/aifi/anaconda3/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/home/aifi/anaconda3/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/aifi/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=[ 'accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['acc'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_acc'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX,\n",
    "    testY), verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with 3 VGG Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 67.910\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ac927d5d6ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0msummarize_diagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# entry point, run the test harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-ac927d5d6ae4>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'> %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# learning curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0msummarize_diagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;31m# entry point, run the test harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ac927d5d6ae4>\u001b[0m in \u001b[0;36msummarize_diagnostics\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cross Entropy Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# plot accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m212\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=[ 'accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['acc'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_acc'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=5, batch_size=32, verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dropout in 3 VGG Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=['accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    print('history', history)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_accuracy'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=100, batch_size=32, validation_data = (testX, testY),verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are auagmenting the data by shifting the height and width and doing a horizontal flip\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=['accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    print('history', history)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_accuracy'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    \n",
    "    # create a Image data generator\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    \n",
    "    # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    \n",
    "    steps=int(trainX.shape[0]/64)\n",
    "    # fit model\n",
    "    history = model.fit(it_train, steps_per_epoch=steps, epochs=10, validation_data = (testX, testY),verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are auagmenting the data by shifting the height and width and doing a horizontal flip.\n",
    "# Dropout is also added to the model\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=['accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    print('history', history)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_accuracy'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    \n",
    "    # create a Image data generator\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    \n",
    "    # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    \n",
    "    steps=int(trainX.shape[0]/64)\n",
    "    # fit model\n",
    "    history = model.fit(it_train, steps_per_epoch=steps, epochs=10, validation_data = (testX, testY),verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout, Data Augmentation and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are auagmenting the data by shifting the height and width and doing a horizontal flip.\n",
    "# Dropout is also added to the model\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=['accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    print('history', history)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_accuracy'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    \n",
    "    # create a Image data generator\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    \n",
    "    # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    \n",
    "    steps=int(trainX.shape[0]/64)\n",
    "    # fit model\n",
    "    history = model.fit(it_train, steps_per_epoch=steps, epochs=400, validation_data = (testX, testY),verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print( '> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are auagmenting the data by shifting the height and width and doing a horizontal flip.\n",
    "# Dropout is also added to the model\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "    # scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same' , input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation= 'relu' , kernel_initializer= 'he_uniform' ,\n",
    "    padding= 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' , kernel_initializer= 'he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    # add dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation= 'softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=['accuracy'])\n",
    "    return model\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    print('history', history)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color= 'blue' , label= 'train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color= 'blue' , label= 'train' )\n",
    "    pyplot.plot(history.history['val_accuracy'], color= 'orange' , label= 'test' )\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    \n",
    "    # create a Image data generator\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    \n",
    "    # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    \n",
    "    steps=int(trainX.shape[0]/64)\n",
    "    # fit model\n",
    "    history = model.fit(it_train, steps_per_epoch=steps, epochs=10, validation_data = (testX, testY),verbose=0)\n",
    "    \n",
    "    # save the model\n",
    "    model.save('final_model.h5')\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aifi/anaconda3/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/aifi/anaconda3/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_8_input to have shape (28, 28, 1) but got array with shape (32, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-03acd92dc2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mrun_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-03acd92dc2dc>\u001b[0m in \u001b[0;36mrun_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# predict class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/engine/sequential.pyc\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aifi/anaconda3/lib/python2.7/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_8_input to have shape (28, 28, 1) but got array with shape (32, 32, 3)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_image(filename):\n",
    "    # load image\n",
    "    image = load_img(filename, target_size=(32, 32))\n",
    "    \n",
    "    # convert image to numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # reshape the image\n",
    "    image = image.reshape(1, 32, 32, 3)\n",
    "    \n",
    "    # convert the type\n",
    "    image = image.astype('float32')\n",
    "    \n",
    "    # normalize the image\n",
    "    image = image/255.0\n",
    "    return image\n",
    "\n",
    "# load the image and run the class\n",
    "def run_example():\n",
    "    image = load_image('sample_image-1.png')\n",
    "    \n",
    "    # load model\n",
    "    model = load_model('final_model.h5')\n",
    "    \n",
    "    # predict class\n",
    "    result = model.predict_classes(image)\n",
    "    \n",
    "    print(result[0])\n",
    "    \n",
    "run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
